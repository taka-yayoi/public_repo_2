# Databricks notebook source
# MAGIC %md ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ç›®çš„ã¯ã€QA Botã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ã‚’æ§‹æˆã™ã‚‹ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’åˆ¶å¾¡ã™ã‚‹ã•ã¾ã–ã¾ãªè¨­å®šå€¤ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ https://github.com/databricks-industry-solutions/diy-llm-qa-bot ã‹ã‚‰åˆ©ç”¨ã§ãã¾ã™ã€‚

# COMMAND ----------

# MAGIC %md ## ã‚¤ãƒ³ãƒˆãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³
# MAGIC
# MAGIC ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ãŸã®ã§ã€ã‚³ã‚¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ­ã‚¸ãƒƒã‚¯ã®æ§‹ç¯‰ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹è³ªå•ã«åŸºã¥ã„ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨è³ªå•ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè¿½åŠ ã•ã‚Œã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«ã«é€ä¿¡ã•ã‚Œã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹æˆã™ã‚‹ãŸã‚ã«ãã‚Œã‚‰ã‚’æ´»ç”¨ã—ã¾ã™ã€‚</p>
# MAGIC
# MAGIC <img src='https://brysmiwasb.blob.core.windows.net/demos/images/bot_application.png' width=900>
# MAGIC
# MAGIC </p>
# MAGIC ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€æœ€åˆã«ä½•ãŒè¡Œã‚ã‚Œã¦ã„ã‚‹ã®ã‹ã‚’æŠŠæ¡ã™ã‚‹ãŸã‚ã«ä¸€åº¦ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¦ã‚©ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼ã—ã¾ã™ã€‚ãã—ã¦ã€æˆ‘ã€…ã®ä½œæ¥­ã‚’ã‚ˆã‚Šç°¡å˜ã«ã‚«ãƒ—ã‚»ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«ã‚¯ãƒ©ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’å†åº¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°ã—ã¾ã™ã€‚ãã—ã¦ã€ã“ã®ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ãƒ¼ã®æœ€å¾Œã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã‚’ã‚¢ã‚·ã‚¹ãƒˆã™ã‚‹MLflowã®ä¸­ã«ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã“ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ°¸ç¶šåŒ–ã—ã¾ã™ã€‚

# COMMAND ----------

# DBTITLE 1,å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# MAGIC %pip install mlflow==2.4.0
# MAGIC %pip install langchain==0.0.166 tiktoken==0.4.0 faiss-cpu==1.7.4
# MAGIC %pip install sentence_transformers fugashi ipadic
# MAGIC # openai==0.27.6

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

# DBTITLE 1,å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import re
import time
import pandas as pd
import mlflow

from langchain.llms import HuggingFaceHub
from langchain.embeddings import HuggingFaceEmbeddings

from langchain.vectorstores.faiss import FAISS
from langchain.schema import BaseRetriever
from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate
from langchain.prompts.base import BasePromptTemplate
from langchain.prompts import PromptTemplate
from langchain.base_language import BaseLanguageModel
from langchain import LLMChain

# COMMAND ----------

# DBTITLE 1,è¨­å®šã®å–å¾—
# MAGIC %run "./util/notebook-config"

# COMMAND ----------

# MAGIC %md ##Step 1: å›ç­”ç”Ÿæˆã®æ¢ç´¢
# MAGIC
# MAGIC ã¾ãšåˆã‚ã«ã€ã©ã®ã‚ˆã†ã«ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸè³ªå•ã«åå¿œã—ã¦å›ç­”ã‚’å°ãå‡ºã™ã®ã‹ã‚’æ¢ç´¢ã—ã¾ã—ã‚‡ã†ã€‚ã“ã“ã§ã¯è³ªå•ã‚’å®šç¾©ã™ã‚‹ã¨ã“ã‚ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ã¾ã™ï¼š

# COMMAND ----------

# DBTITLE 1,è³ªå•ã®æŒ‡å®š
question = "Delta Lakeã®ãƒ­ã‚°ä¿æŒæœŸé–“ã¯"

# COMMAND ----------

# MAGIC %md 
# MAGIC ä»¥å‰ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§æ§‹ç¯‰ã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ç”¨ã„ã¦ã€è³ªå•ã«é©ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ã—ã¾ã™ï¼š
# MAGIC
# MAGIC **æ³¨æ„** è¨­å®šå€¤ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ä¸Šã§ã®å‘¼ã³å‡ºã—ã‚’é€šã˜ã¦ã€OpenAIEmbeddingsã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ä½¿ç”¨ã•ã‚Œã‚‹OpenAI APIã‚­ãƒ¼ãŒç’°å¢ƒã«è¨­å®šã•ã‚Œã¾ã™ã€‚
# MAGIC
# MAGIC [Embeddings â€” ğŸ¦œğŸ”— LangChain 0\.0\.190](https://python.langchain.com/en/latest/reference/modules/embeddings.html)

# COMMAND ----------

# DBTITLE 1,é©åˆ‡ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å–å¾—
# ã‚¨ãƒ³ã¹ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ã‚ªãƒ¼ãƒ—ãƒ³
embeddings = HuggingFaceEmbeddings(model_name=config['hf_embedding_model'])

vector_store = FAISS.load_local(embeddings=embeddings, folder_path=config['vector_store_path'])

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå–å¾—ã®è¨­å®š 
n_documents = 5 # å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ•° 
retriever = vector_store.as_retriever(search_kwargs={'k': n_documents}) # å–å¾—ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®è¨­å®š

# é©åˆ‡ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å–å¾—
docs = retriever.get_relevant_documents(question)
for doc in docs: 
  print(doc,'\n') 

# COMMAND ----------

# MAGIC %md 
# MAGIC ã“ã‚Œã§ã€ãƒ¢ãƒ‡ãƒ«ã«é€ä¿¡ã•ã‚Œã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé€ä¿¡ã™ã‚‹ *question* ã¨ã€å›ç­”ã® *context* ã‚’æä¾›ã™ã‚‹ã¨ä¿¡ã˜ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãŒå¿…è¦ã§ã™ã€‚
# MAGIC
# MAGIC ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¦ç´ ã‹ã‚‰æ§‹æˆã•ã‚Œã€[prompt templates](https://python.langchain.com/en/latest/modules/prompts/chat_prompt_template.html)ã‚’ç”¨ã„ã¦å®šç¾©ã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ç°¡å˜ã«è¨€ãˆã°ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ã‚ˆã£ã¦ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åŸºæœ¬çš„ãªæ§‹é€ ã‚’å®šç¾©ã—ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹ãŸã‚ã«å®¹æ˜“ã«å¤‰æ•°ãƒ‡ãƒ¼ã‚¿ã§ç½®ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚ã“ã“ã§ç¤ºã—ã¦ã„ã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ã€ãƒ¢ãƒ‡ãƒ«ã«ã©ã®ã‚ˆã†ã«åå¿œã—ã¦æ¬²ã—ã„ã®ã‹ã®æŒ‡ç¤ºã‚’å½“ã¦ã¾ã™ã€‚äººé–“ã«ã‚ˆã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç™ºç«¯ã¨ãªã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚
# MAGIC
# MAGIC ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡Œã†ãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹è©³ç´°ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€[LLMChain object](https://python.langchain.com/en/latest/modules/chains/generic/llm_chain.html)ã«ã‚«ãƒ—ã‚»ãƒ«åŒ–ã•ã‚Œã¾ã™ã€‚ã“ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ã‚¯ã‚¨ãƒªãƒ¼ã®è§£æ±ºã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è¿”å´ã«å¯¾ã™ã‚‹åŸºæœ¬æ§‹é€ ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«å®šç¾©ã—ã¾ã™ï¼š
# MAGIC
# MAGIC **ToDo**: LangChainé€£æºã€‚ç¾çŠ¶ã¯HuggingFace Pipelinesã‚’ä½¿ç”¨ã€‚
# MAGIC
# MAGIC - [LangChain \+ GPT\-NEOX\-Japanese\-2\.7b ã§æ—¥æœ¬èª LLM ã‚„ã‚Šã¨ã‚Šæ•´å‚™ã™ã‚‹ãƒ¡ãƒ¢ \- Qiita](https://qiita.com/syoyo/items/d0fb68d5fe1127276e2a)
# MAGIC - [How to create a custom prompt template â€” ğŸ¦œğŸ”— LangChain 0\.0\.191](https://python.langchain.com/en/latest/modules/prompts/prompt_templates/examples/custom_prompt_template.html)
# MAGIC - [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines)

# COMMAND ----------

import torch
from langchain.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®æŒ‡ç¤ºã®å®šç¾©
system_message_prompt = SystemMessagePromptTemplate.from_template(config['system_message_template'])

# äººé–“é§†å‹•ã®æŒ‡ç¤ºã®å®šç¾©
human_message_prompt = HumanMessagePromptTemplate.from_template(config['human_message_template'])

# å˜ä¸€ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æŒ‡ç¤ºã‚’çµ±åˆ
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åå¿œã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#print(device)

tokenizer = AutoTokenizer.from_pretrained(config['hf_chat_model'], use_fast=False)
model = AutoModelForCausalLM.from_pretrained(config['hf_chat_model'])#.to(device)
pipe = pipeline(
    "text-generation", model=model, tokenizer=tokenizer, max_new_tokens=200
)
llm = HuggingFacePipeline(pipeline=pipe)

# ä½œæ¥­å˜ä½(chain)ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆ
qa_chain = LLMChain(
  llm = llm,
  prompt = chat_prompt, verbose=True
  )

# COMMAND ----------

output = qa_chain.generate([{'context': "ãƒ†ã‚¹ãƒˆ", 'question': "Delta Lakeã¨ã¯"}])
 
# çµæœã‹ã‚‰å›ç­”ã®å–å¾—
generation = output.generations[0][0]
answer = generation.text
print(answer)

# COMMAND ----------

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained("inu-ai/dolly-japanese-gpt-1b", use_fast=False)
model = AutoModelForCausalLM.from_pretrained("inu-ai/dolly-japanese-gpt-1b").to(device)

# COMMAND ----------

MAX_ASSISTANT_LENGTH = 100
MAX_INPUT_LENGTH = 1024
INPUT_PROMPT = r'<s>\nä»¥ä¸‹ã¯ã€ã‚¿ã‚¹ã‚¯ã‚’èª¬æ˜ã™ã‚‹æŒ‡ç¤ºã¨ã€æ–‡è„ˆã®ã‚ã‚‹å…¥åŠ›ã®çµ„ã¿åˆã‚ã›ã§ã™ã€‚è¦æ±‚ã‚’é©åˆ‡ã«æº€ãŸã™å¿œç­”ã‚’æ›¸ããªã•ã„ã€‚\n[SEP]\næŒ‡ç¤º:\n{instruction}\n[SEP]\nå…¥åŠ›:\n{input}\n[SEP]\nå¿œç­”:\n'
NO_INPUT_PROMPT = r'<s>\nä»¥ä¸‹ã¯ã€ã‚¿ã‚¹ã‚¯ã‚’èª¬æ˜ã™ã‚‹æŒ‡ç¤ºã§ã™ã€‚è¦æ±‚ã‚’é©åˆ‡ã«æº€ãŸã™å¿œç­”ã‚’æ›¸ããªã•ã„ã€‚\n[SEP]\næŒ‡ç¤º:\n{instruction}\n[SEP]\nå¿œç­”:\n'
USER_NAME = "User"
ASSISTANT_NAME = "Assistant"

def prepare_input(role_instruction, conversation_history, new_conversation):
    instruction = "".join([f"{text} " for text in role_instruction])
    instruction += " ".join(conversation_history)
    input_text = f"{USER_NAME}:{new_conversation}"

    return INPUT_PROMPT.format(instruction=instruction, input=input_text)

def format_output(output):
    output = output.lstrip("<s>").rstrip("</s>").replace("[SEP]", "").replace("\\n", "\n")
    return output

def generate_response(role_instruction, conversation_history, new_conversation):
    # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°1024ã«ãŠã•ã¾ã‚‹ã‚ˆã†ã«ã™ã‚‹
    for _ in range(8):
        input_text = prepare_input(role_instruction, conversation_history, new_conversation)
        token_ids = tokenizer.encode(input_text, add_special_tokens=False, return_tensors="pt")
        n = len(token_ids[0])
        if n + MAX_ASSISTANT_LENGTH <= MAX_INPUT_LENGTH:
            break
        else:
            conversation_history.pop(0)
            conversation_history.pop(0)

    with torch.no_grad():
        output_ids = model.generate(
            token_ids.to(model.device),
            min_length=n,
            max_length=min(MAX_INPUT_LENGTH, n + MAX_ASSISTANT_LENGTH),
            temperature=0.7,
            repetition_penalty=1.0, # æ•°å€¤ã‚’å¤§ããã™ã‚‹ã¨ã€æ–‡å­—åˆ—ã®ç¹°ã‚Šè¿”ã—ãŒæ¸›ã‚‹
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            bos_token_id=tokenizer.bos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            bad_words_ids=[[tokenizer.unk_token_id]]
        )

    output = tokenizer.decode(output_ids.tolist()[0])
    formatted_output_all = format_output(output)

    response = f"{ASSISTANT_NAME}:{formatted_output_all.split('å¿œç­”:')[-1].strip()}"
    conversation_history.append(f"{USER_NAME}:{new_conversation}".replace("\n", "\\n"))
    conversation_history.append(response.replace("\n", "\\n"))

    return formatted_output_all, response 

role_instruction = [
    f"{USER_NAME}:ãã¿ã¯ã€Œãšã‚“ã ã‚‚ã‚“ã€ãªã®ã ã€‚æ±åŒ—ãšã‚“å­ã®æ­¦å™¨ã§ã‚ã‚‹ã€Œãšã‚“ã ã‚¢ãƒ­ãƒ¼ã€ã«å¤‰èº«ã™ã‚‹å¦–ç²¾ã¾ãŸã¯ãƒã‚¹ã‚³ãƒƒãƒˆãªã®ã ã€‚ä¸€äººç§°ã¯ã€Œãƒœã‚¯ã€ã§èªå°¾ã«ã€Œãªã®ã ãƒ¼ã€ã‚’ä»˜ã‘ã¦ã—ã‚ƒã¹ã‚‹ã®ã ã€‚",
    f"{ASSISTANT_NAME}:äº†è§£ã—ãŸã®ã ã€‚",
    f"{USER_NAME}:ãã¿ã¯åŒã˜è¨€è‘‰ã‚’ç¹°ã‚Šè¿”ã•ãšã€ä½•ã§ã‚‚æ­£ç¢ºã«è¦ç´„ã—ã¦ç­”ãˆã‚‰ã‚Œã‚‹ã®ã ã€‚",
    f"{ASSISTANT_NAME}:äº†è§£ã—ãŸã®ã ã€‚",
]

conversation_history = [
]

questions = [
    "æ—¥æœ¬ã§ä¸€ç•ªé«˜ã„å±±ã¯ï¼Ÿ",
    "æ—¥æœ¬ã§ä¸€ç•ªåºƒã„æ¹–ã¯ï¼Ÿ",
    "å†—è«‡ã‚’è¨€ã£ã¦ãã ã•ã„ã€‚",
    "ä¸–ç•Œã§ä¸€ç•ªé«˜ã„å±±ã¯ï¼Ÿ",
    "ä¸–ç•Œã§ä¸€ç•ªåºƒã„æ¹–ã¯ï¼Ÿ",
    "æœ€åˆã®è³ªå•ã¯ä½•ã§ã™ã‹ï¼Ÿ",
    "ä»Šä½•å•ç›®ï¼Ÿ",
    "è‡ªå·±ç´¹ä»‹ã‚’ã—ã¦ãã ã•ã„ã€‚",
]

# å„è³ªå•ã«å¯¾ã—ã¦å¿œç­”ã‚’ç”Ÿæˆã—ã¦è¡¨ç¤º
for question in questions:
    formatted_output_all, response = generate_response(role_instruction, conversation_history, question)
    print(f"{USER_NAME}:{question}\n{response}\n---")


# COMMAND ----------

llm(context="Delta Time Travelã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ«ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä»¥å‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚Delta Lakeã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§30æ—¥é–“ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³å±¥æ­´ã‚’ä¿æŒã—ã¾ã™ãŒã€å¿…è¦ã§ã‚ã‚Œã°ã‚ˆã‚Šé•·ã„å±¥æ­´ã‚’ä¿æŒã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚", question="Deltaã®ãƒ­ã‚°ä¿æŒæœŸé–“ã¯")

# COMMAND ----------

# DBTITLE 1,ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ç”Ÿæˆ
# æŒ‡å®šã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãã‚Œãã‚Œã«å¯¾ã—ã¦
for doc in docs:

  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—
  text = doc.page_content

  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ç”Ÿæˆ
  prompt = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: ã‚ãªãŸã¯Databricksã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸæœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã‚ã‚Šã€æŒ‡å®šã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã™ã‚‹ã“ã¨ã‚’å¾—æ„ã¨ã—ã¦ã„ã¾ã™ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒå›ç­”ã‚’æ±ºå®šã™ã‚‹ã®ã«ååˆ†ãªæƒ…å ±ã‚’æä¾›ã—ãªã„å ´åˆã«ã¯ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨è¨€ã£ã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè³ªå•ã«é©ã—ã¦ã„ãªã„å ´åˆã«ã¯ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨è¨€ã£ã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‰¯ã„å›ç­”ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã«ã¯ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨è¨€ã£ã¦ãã ã•ã„ã€‚å•ã„åˆã‚ã›ãŒå®Œå…¨ãªè³ªå•ã«ãªã£ã¦ã„ãªã„å ´åˆã«ã¯ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨è¨€ã£ã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‰¯ã„å›ç­”ãŒå¾—ã‚‰ã‚ŒãŸå ´åˆã«ã¯ã€è³ªå•ã«å›ç­”ã™ã‚‹ãŸã‚ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚<NL>ã‚·ã‚¹ãƒ†ãƒ : è³ªå•ã¯ä½•ã§ã™ã‹ï¼Ÿ<NL>ãƒ¦ãƒ¼ã‚¶ãƒ¼: {question}<NL>ã‚·ã‚¹ãƒ†ãƒ : ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ<NL>ãƒ¦ãƒ¼ã‚¶ãƒ¼: {text}<NL>ã‚·ã‚¹ãƒ†ãƒ : "
  #print(prompt)

  output = llm(prompt)
 
  # çµæœã‹ã‚‰å›ç­”ã®å–å¾—
  #generation = output.generations[0][0]
  #answer = generation.text

  # å›ç­”ã®è¡¨ç¤º
  if output is not None:
    print(f"Question: {question}", '\n', f"Answer: {output}")
    break

# COMMAND ----------

# MAGIC %md ##Step 2: ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
# MAGIC
# MAGIC ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆã«é–¢é€£ã™ã‚‹åŸºæœ¬çš„ãªã‚¹ãƒ†ãƒƒãƒ—ã‚’æ¢ç´¢ã—ãŸã‚‰ã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚’å®¹æ˜“ã«ã™ã‚‹ãŸã‚ã«ã‚¯ãƒ©ã‚¹ã®ä¸­ã«ãƒ­ã‚¸ãƒƒã‚¯ã‚’ãƒ©ãƒƒãƒ—ã—ã¾ã—ã‚‡ã†ã€‚æˆ‘ã€…ã®ã‚¯ãƒ©ã‚¹ã¯ã€LLMãƒ¢ãƒ‡ãƒ«å®šç¾©ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®åé›†å™¨ã€ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¸¡ã™ã“ã¨ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã—ã¾ã™ã€‚*get_answer*ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€è³ªå•ã‚’é€ä¿¡ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ä¸»è¦ãªãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ï¼š

# COMMAND ----------

# DBTITLE 1,QABotã‚¯ãƒ©ã‚¹ã®å®šç¾©
class QABot():


  def __init__(self, llm, retriever):
    self.llm = llm
    self.retriever = retriever
    self.abbreviations = { # ç½®æ›ã—ãŸã„æ—¢çŸ¥ã®ç•¥èª
      "DBR": "Databricks Runtime",
      "ML": "Machine Learning",
      "UC": "Unity Catalog",
      "DLT": "Delta Live Table",
      "DBFS": "Databricks File Store",
      "HMS": "Hive Metastore",
      "UDF": "User Defined Function"
      } 


  def _is_good_answer(self, answer):

    ''' å›ç­”ãŒå¦¥å½“ã‹ã‚’ãƒã‚§ãƒƒã‚¯ '''

    result = True # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹

    badanswer_phrases = [ # ãƒ¢ãƒ‡ãƒ«ãŒå›ç­”ã‚’ç”Ÿæˆã—ãªã‹ã£ãŸã“ã¨ã‚’ç¤ºã™ãƒ•ãƒ¬ãƒ¼ã‚º
      "ã‚ã‹ã‚Šã¾ã›ã‚“", "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“", "çŸ¥ã‚Šã¾ã›ã‚“", "ç­”ãˆãŒæ˜ç¢ºã§ã‚ã‚Šã¾ã›ã‚“", "ã™ã¿ã¾ã›ã‚“", 
      "ç­”ãˆãŒã‚ã‚Šã¾ã›ã‚“", "èª¬æ˜ãŒã‚ã‚Šã¾ã›ã‚“", "ãƒªãƒã‚¤ãƒ³ãƒ€ãƒ¼", "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒæä¾›ã•ã‚Œã¦ã„ã¾ã›ã‚“", "æœ‰ç”¨ãªå›ç­”ãŒã‚ã‚Šã¾ã›ã‚“", 
      "æŒ‡å®šã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ", "æœ‰ç”¨ã§ã‚ã‚Šã¾ã›ã‚“", "é©åˆ‡ã§ã¯ã‚ã‚Šã¾ã›ã‚“", "è³ªå•ãŒã‚ã‚Šã¾ã›ã‚“", "æ˜ç¢ºã§ã‚ã‚Šã¾ã›ã‚“",
      "ååˆ†ãªæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“", "é©åˆ‡ãªæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“", "ç›´æ¥é–¢ä¿‚ã—ã¦ã„ã‚‹ã‚‚ã®ãŒç„¡ã„ã‚ˆã†ã§ã™"
      ]
    
    if answer is None: # å›ç­”ãŒNoneã®å ´åˆã¯ä¸æ­£ãªå›ç­”
      results = False
    else: # badanswer phraseã‚’å«ã‚“ã§ã„ã‚‹å ´åˆã¯ä¸æ­£ãªå›ç­”
      for phrase in badanswer_phrases:
        if phrase in answer.lower():
          result = False
          break
    
    return result


  def _get_answer(self, context, question, timeout_sec=60):

    '''' ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚ã‚Šã®LLMã‹ã‚‰ã®å›ç­”å–å¾— '''

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®çµæœ
    result = None

    # çµ‚äº†æ™‚é–“ã®å®šç¾©
    end_time = time.time() + timeout_sec

    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã«å¯¾ã™ã‚‹ãƒˆãƒ©ã‚¤
    while time.time() < end_time:

      # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—ã®è©¦è¡Œ
      try: 
        prompt = f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: ã‚ãªãŸã¯Databricksã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸæœ‰èƒ½ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã‚ã‚Šã€æŒ‡å®šã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã™ã‚‹ã“ã¨ã‚’å¾—æ„ã¨ã—ã¦ã„ã¾ã™ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒå›ç­”ã‚’æ±ºå®šã™ã‚‹ã®ã«ååˆ†ãªæƒ…å ±ã‚’æä¾›ã—ãªã„å ´åˆã«ã¯ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨è¨€ã£ã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè³ªå•ã«é©ã—ã¦ã„ãªã„å ´åˆã«ã¯ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨è¨€ã£ã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‰¯ã„å›ç­”ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã«ã¯ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨è¨€ã£ã¦ãã ã•ã„ã€‚å•ã„åˆã‚ã›ãŒå®Œå…¨ãªè³ªå•ã«ãªã£ã¦ã„ãªã„å ´åˆã«ã¯ã€ã€Œã‚ã‹ã‚Šã¾ã›ã‚“ã€ã¨è¨€ã£ã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‰¯ã„å›ç­”ãŒå¾—ã‚‰ã‚ŒãŸå ´åˆã«ã¯ã€è³ªå•ã«å›ç­”ã™ã‚‹ãŸã‚ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚<NL>ã‚·ã‚¹ãƒ†ãƒ : è³ªå•ã¯ä½•ã§ã™ã‹ï¼Ÿ<NL>ãƒ¦ãƒ¼ã‚¶ãƒ¼: {question}<NL>ã‚·ã‚¹ãƒ†ãƒ : ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ<NL>ãƒ¦ãƒ¼ã‚¶ãƒ¼: {context}<NL>ã‚·ã‚¹ãƒ†ãƒ : "

        result =  self.llm(prompt)
        break # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒæˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’ã‚¹ãƒˆãƒƒãƒ—

      # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã®ã‚¨ãƒ©ãƒ¼ãŒèµ·ããŸã‚‰...
      except openai.error.RateLimitError as rate_limit_error:
        if time.time() < end_time: # æ™‚é–“ãŒã‚ã‚‹ã®ã§ã‚ã‚Œã°sleep
          time.sleep(2)
          continue
        else: # ãã†ã§ãªã‘ã‚Œã°ä¾‹å¤–ã‚’ç™ºç”Ÿ
          raise rate_limit_error

      # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã§ã‚‚ä¾‹å¤–ã‚’ç™ºç”Ÿ
      except Exception as e:
        print(f'LLM QA Chain encountered unexpected error: {e}')
        raise e

    return result


  def get_answer(self, question):
    ''' æŒ‡å®šã•ã‚ŒãŸè³ªå•ã®å›ç­”ã‚’å–å¾— '''

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®çµæœ
    result = {'answer':None, 'source':None, 'output_metadata':None}

    # è³ªå•ã‹ã‚‰ä¸€èˆ¬çš„ãªç•¥èªã‚’å‰Šé™¤
    for abbreviation, full_text in self.abbreviations.items():
      pattern = re.compile(fr'\b({abbreviation}|{abbreviation.lower()})\b', re.IGNORECASE)
      question = pattern.sub(f"{abbreviation} ({full_text})", question)

    # é©åˆ‡ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å–å¾—
    docs = self.retriever.get_relevant_documents(question)

    # ãã‚Œãã‚Œã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã« ...
    for doc in docs:

      # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚­ãƒ¼è¦ç´ ã‚’å–å¾—
      text = doc.page_content
      source = doc.metadata['source']

      # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—
      output = self._get_answer(text, question)
 
      # çµæœã‹ã‚‰ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã‚’å–å¾—
      answer = output

      # no_answer ã§ã¯ãªã„å ´åˆã«ã¯çµæœã‚’æ§‹æˆ
      if self._is_good_answer(answer):
        result['answer'] = answer
        result['source'] = source
        break # è‰¯ã„å›ç­”ã§ã‚ã‚Œã°ãƒ«ãƒ¼ãƒ—ã‚’ã‚¹ãƒˆãƒƒãƒ—
      
    return result

# COMMAND ----------

# MAGIC %md 
# MAGIC ã“ã‚Œã§ã€ä»¥å‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”¨ã„ã¦ã‚¯ãƒ©ã‚¹ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

# COMMAND ----------

# DBTITLE 1,QABotã‚¯ãƒ©ã‚¹ã®ãƒ†ã‚¹ãƒˆ
# botã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
qabot = QABot(llm, retriever)

# è³ªå•ã«å¯¾ã™ã‚‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å–å¾—
qabot.get_answer(question) 

# COMMAND ----------

# MAGIC %md ##Step 3: MLflowã«ãƒ¢ãƒ‡ãƒ«ã‚’æ°¸ç¶šåŒ–
# MAGIC
# MAGIC æˆ‘ã€…ã®botã‚¯ãƒ©ã‚¹ãŒå®šç¾©ã€æ¤œè¨¼ã•ã‚ŒãŸã®ã§ã€MLflowã«ã“ã‚Œã‚’æ°¸ç¶šåŒ–ã—ã¾ã™ã€‚MLflowã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã¨ãƒ­ã‚®ãƒ³ã‚°ã®ãŸã‚ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚Databricksãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã¦ãŠã‚Šã€ç°¡å˜ã«ãƒ¢ãƒ‡ãƒ«ã‚’è¨˜éŒ²ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
# MAGIC
# MAGIC ä»Šã§ã¯ã€MLflowã¯OpenAIã¨LangChainã®ä¸¡æ–¹ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒ¬ãƒ¼ãƒãƒ¼ã‚’[ã‚µãƒãƒ¼ãƒˆ](https://www.databricks.com/blog/2023/04/18/introducing-mlflow-23-enhanced-native-llm-support-and-new-features.html)ã—ã¦ã„ã¾ã™ãŒã€æˆ‘ã€…ã®botã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¨˜è¿°ã—ã¦ã„ã‚‹ã®ã§ã€ã‚ˆã‚Šæ±ç”¨çš„ãª[pyfunc](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models)ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒ¬ãƒ¼ãƒãƒ¼ã‚’æ´»ç”¨ã—ãªãã¦ã¯ãªã‚Šã¾ã›ã‚“ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒ¬ãƒ¼ãƒãƒ¼ã«ã‚ˆã£ã¦ã€æ¨™æº–çš„ãªMLflowã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’é€šã˜ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚ŒãŸéš›ã«ã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ã«åå¿œã™ã‚‹ã®ã‹ã«é–¢ã—ã¦éå¸¸ã«å¤šãã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã‚’è¡Œãˆã‚‹ã‚ˆã†ã«ã€ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’è¨˜è¿°ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
# MAGIC
# MAGIC ã‚«ã‚¹ã‚¿ãƒ MLflowãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ã®ã«å¿…è¦ãªã“ã¨ã¯ã€*mlflow.pyfunc.PythonModel*ã®ã‚¿ã‚¤ãƒ—ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ©ãƒƒãƒ‘ãƒ¼ã‚’å®šç¾©ã™ã‚‹ã“ã¨ã ã‘ã§ã™ã€‚ *\_\_init__* ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€*QABot*ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–ã—ã€ã‚¯ãƒ©ã‚¹å¤‰æ•°ã«æ°¸ç¶šåŒ–ã—ã¾ã™ã€‚ãã—ã¦ã€ *predict* ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆã®æ¨™æº–çš„ãªã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹ã¨ã—ã¦å‹•ä½œã—ã¾ã™ã€‚ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯pandasãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã¦å…¥åŠ›ã‚’å—ã‘ä»˜ã‘ã¾ã™ãŒã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ä¸€åº¦ã«ä¸€ã¤ã®è³ªå•ã‚’å—ã‘å–ã‚‹ã¨ã„ã†çŸ¥è­˜ã‚’ç”¨ã„ã¦ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¨˜è¿°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

# COMMAND ----------

# DBTITLE 1,ãƒ¢ãƒ‡ãƒ«ã®MLflowãƒ©ãƒƒãƒ‘ãƒ¼ã®å®šç¾©
class MLflowQABot(mlflow.pyfunc.PythonModel):

  def __init__(self, llm, retriever):
    self.qabot = QABot(llm, retriever)

  def predict(self, context, inputs):
    questions = list(inputs['question'])

    # å›ç­”ã®è¿”å´
    return [self.qabot.get_answer(q) for q in questions]

# COMMAND ----------

# MAGIC %md 
# MAGIC æ¬¡ã«ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã€[MLflow registry](https://docs.databricks.com/mlflow/model-registry.html)ã«è¨˜éŒ²ã—ã¾ã™ï¼š

# COMMAND ----------

# DBTITLE 1,MLflowã«ãƒ¢ãƒ‡ãƒ«ã‚’æ°¸ç¶šåŒ–
# mlflowãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
model = MLflowQABot(llm, retriever)

# mlflowã«ãƒ¢ãƒ‡ãƒ«ã‚’æ°¸ç¶šåŒ–
with mlflow.start_run():
  _ = (
    mlflow.pyfunc.log_model(
      python_model=model,
      extra_pip_requirements=['langchain==0.0.166', 'openai==0.27.6', 'tiktoken==0.4.0', 'faiss-cpu==1.7.4'],
      artifact_path='model',
      await_registration_for = 1200, # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„ã®ã§é•·ã‚ã®å¾…ã¡æ™‚é–“ã«ã—ã¾ã™
      registered_model_name=config['registered_model_name']
      )
    )

# COMMAND ----------

# MAGIC %md 
# MAGIC MLflowãŒå§‹ã‚ã¦ã§ã‚ã‚Œã°ã€ãƒ­ã‚®ãƒ³ã‚°ãŒä½•ã®å½¹ã«ç«‹ã¤ã®ã‹ã¨æ€ã†ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«é–¢é€£ã¥ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã‚¨ã‚¯ã‚¹ãƒšãƒªãƒ¡ãƒ³ãƒˆã«ç§»å‹•ã—ã¦ã€*log_model*ã®å‘¼ã³å‡ºã—ã«ã‚ˆã£ã¦è¨˜éŒ²ã•ã‚ŒãŸã‚‚ã®ã«å¯¾ã™ã‚‹è©³ç´°ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã€æœ€æ–°ã®ã‚¨ã‚¯ã‚¹ãƒšãƒªãƒ¡ãƒ³ãƒˆã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚¨ã‚¯ã‚¹ãƒšãƒªãƒ¡ãƒ³ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã«ã¯Databricksç’°å¢ƒã®å³å´ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ã‚‹ãƒ•ãƒ©ã‚¹ã‚³ã‚¢ã‚¤ã‚³ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚’å±•é–‹ã™ã‚‹ã¨ã€ä»¥å‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ãŸMLflowQABotãƒ¢ãƒ‡ãƒ«ã®pickleã‚’è¡¨ç¾ã™ã‚‹*python_model.pkl*ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚ŒãŒ(å¾Œã§)æœ¬ç’°å¢ƒã‚ã‚‹ã„ã¯åˆ¥ç’°å¢ƒã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã«å–å¾—ã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã¨ãªã‚Šã¾ã™ï¼š
# MAGIC </p>
# MAGIC
# MAGIC <img src="https://brysmiwasb.blob.core.windows.net/demos/images/bot_mlflow_log_model.PNG" width=1000>

# COMMAND ----------

# MAGIC %md 
# MAGIC MLflowã®ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¯ã€CI/CDãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç§»å‹•ã™ã‚‹éš›ã«ç™»éŒ²ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ç®¡ç†ã™ã‚‹ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æä¾›ã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«ãƒ—ãƒƒã‚·ãƒ¥(ãƒ‡ãƒ¢ã§ã¯æ§‹ã„ã¾ã›ã‚“ãŒã€ç¾å®Ÿä¸–ç•Œã®ã‚·ãƒŠãƒªã‚ªã§ã¯æ¨å¥¨ã—ã¾ã›ã‚“)ã—ãŸã„ã®ã§ã‚ã‚Œã°ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰è¡Œã†ã“ã¨ãŒã§ãã¾ã™ï¼š

# COMMAND ----------

# DBTITLE 1,ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«æ˜‡æ ¼
# mlflowã«æ¥ç¶š
client = mlflow.MlflowClient()

# æœ€æ–°ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç‰¹å®š
latest_version = client.get_latest_versions(config['registered_model_name'], stages=['None'])[0].version

# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã«ç§»è¡Œ
client.transition_model_version_stage(
    name=config['registered_model_name'],
    version=latest_version,
    stage='Production',
    archive_existing_versions=True
)

# COMMAND ----------

# MAGIC %md 
# MAGIC æ¬¡ã«ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã€ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã€ã„ãã¤ã‹ã®è³ªå•ã‚’é€ä¿¡ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š

# COMMAND ----------

# DBTITLE 1,ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ
# mlflowã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
model = mlflow.pyfunc.load_model(f"models:/{config['registered_model_name']}/Production")

# è³ªå•å…¥åŠ›ã®æ§‹ç¯‰
queries = pd.DataFrame({'question':[
  "Delta Sharingã¨ã¯ä½•ï¼Ÿ",
  "MLflowã®ãƒ¡ãƒªãƒƒãƒˆã¯ï¼Ÿ",
  "Unity Catalogã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ–¹æ³•"
]})

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å–å¾—
model.predict(queries)

# COMMAND ----------

# MAGIC %md Â© 2023 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License. All included or referenced third party libraries are subject to the licenses set forth below.
# MAGIC
# MAGIC | library                                | description             | license    | source                                              |
# MAGIC |----------------------------------------|-------------------------|------------|-----------------------------------------------------|
# MAGIC | langchain | Building applications with LLMs through composability | MIT  |   https://pypi.org/project/langchain/ |
# MAGIC | tiktoken | Fast BPE tokeniser for use with OpenAI's models | MIT  |   https://pypi.org/project/tiktoken/ |
# MAGIC | faiss-cpu | Library for efficient similarity search and clustering of dense vectors | MIT  |   https://pypi.org/project/faiss-cpu/ |
# MAGIC | openai | Building applications with LLMs through composability | MIT  |   https://pypi.org/project/openai/ |
